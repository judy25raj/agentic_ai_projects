# â­ Agentic AI Systems & Cloud Observability

**Author:** Judy Raj  

This repository contains **production-style Agentic AI and Cloud Observability projects** demonstrating how modern AI systems can be designed to be **grounded, evaluable, observable, and operationally reliable**.

The focus is on **real-world engineering practices**, not toy demos:
- Clear agent responsibilities  
- Deterministic and structured outputs  
- Evaluation and quality control  
- Secure configuration  
- End-to-end observability  

These projects reflect how **AI systems are built, monitored, and trusted in production environments**.

---

## ğŸ§  What This Portfolio Demonstrates

- Agentic AI workflows (multi-agent orchestration)
- Retrieval-Augmented Generation (RAG)
- Automated evaluation using Judge Agents
- Cloud-native deployment on Azure
- Observability-first design (logs, metrics, traces)
- Secure configuration and production hygiene
- Interview-ready documentation and structure

---

## ğŸ“Œ Featured Projects

---

## ğŸ”¹ Project 1: Agentic AI â€” PDF RAG + Judge Agent  
ğŸ“ **Folder:** `agentic-ai-pdf-rag-judge`

### Problem Addressed
LLM-based systems must produce **grounded answers** and provide **confidence in output quality**. This project demonstrates how multiple AI agents collaborate to ensure accuracy, relevance, and traceability when answering questions from documents.

### Architecture Flow
- PDF documents are ingested and chunked  
- A **Retriever Agent** performs vector-based semantic search  
- A **Generator Agent** produces a grounded answer using retrieved context  
- A **Judge Agent** evaluates answer quality and relevance  
- Structured JSON outputs are produced for scoring and traceability  
- *(Optional)* Traces and spans are emitted to **Langfuse** for observability  

### Key Capabilities
- Multi-agent workflow (Retriever â†’ Generator â†’ Judge)
- Local vector embeddings and semantic search
- LLM-driven grounded answer generation
- Automated evaluation and scoring
- Deterministic, structured outputs
- Optional observability with Langfuse

### Tech & Skills
**Python Â· Agentic AI Â· RAG Â· Vector Embeddings Â· LLM Evaluation Â· Prompt Engineering Â· Observability**

ğŸ“Œ **Status:** âœ… Complete (Portfolio / Interview-ready)

---

## ğŸ”¹ Project 2: Atomic Agent on Azure with Elastic Observability  
ğŸ“ **Folder:** `atomic-agent-azure-elastic-observability`

### Problem Addressed
AI agents running in production must be **observable, debuggable, and auditable**. This project focuses on **operational visibility** rather than model accuracy alone.

### Architecture Flow
- Atomic agent runs on an Azure Linux VM  
- Agent emits structured logs and system metrics  
- Data flows through Elastic ingest pipelines  
- Elasticsearch indexes the telemetry  
- Kibana dashboards provide real-time visibility  

### Key Capabilities
- Atomic agent execution model
- Azure VMâ€“based deployment
- Structured JSON logging
- Centralized metrics and dashboards
- Infrastructure and observability views in Kibana
- Production-style telemetry pipeline

### Tech & Skills
**Azure Â· Elastic Stack Â· Observability Â· Cloud Operations Â· Automation Â· Platform Engineering**

ğŸ“Œ **Status:** âœ… Complete (Portfolio / Interview-ready)

---

## ğŸ“ Repository Structure

```
agentic_ai_projects/
â”œâ”€â”€ agentic-ai-pdf-rag-judge/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/              # gitignored
â”‚   â””â”€â”€ .env.example
â”‚
â”œâ”€â”€ atomic-agent-azure-elastic-observability/
â”‚   â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ elastic/
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ screenshots/
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸ” Security & Configuration

- Sensitive values are **never committed**
- Environment variables are managed using `.env` files
- Each project includes a `.env.example` template
- `.gitignore` enforces safe portfolio practices

---

## â–¶ï¸ How to Use This Repository

Each project is **self-contained**.

1. Navigate into a project folder  
2. Read the project-specific `README.md`  
3. Follow setup and execution instructions  

This allows reviewers to explore projects independently without confusion.

---

## ğŸ‘©â€ğŸ’» About the Author

Senior Platform & Automation Engineer with extensive experience in:
- Enterprise application development  
- Automation and production support  
- Cloud infrastructure and observability  
- Agentic AI system design and evaluation  

Currently focused on **Agentic AI architectures**, **AI evaluation frameworks**, **Python automation**, and **cloud observability** in regulated and production-grade environments.

This repository emphasizes **implementation-focused engineering** over academic demonstrations.

---

## ğŸ Portfolio Notes

- All projects are **complete and intentional**
- Code and documentation reflect **production thinking**
- Designed for **interviews, technical discussions, and hiring reviews**
